{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Imapcts\n",
    "The code in this notebook is used to demonstrate the performance impacts produced by training a model using Differentially Private gradient optimization techniques \n",
    "using PyTorch and Opacus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This cell changes display settings for the notebook.\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import neccesary python modules\n",
    "from opacus.utils import module_modification\n",
    "from opacus.dp_model_inspector import DPModelInspector\n",
    "from opacus import PrivacyEngine\n",
    "from random import randint\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This cell contains some helper functions used to help train and visualize model training.\n",
    "# helper function to show an image\n",
    "# (used in the `plot_classes_preds` function below)\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.cpu().numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "def select_n_random(data, labels, n=100):\n",
    "    '''\n",
    "    Selects n random datapoints and their corresponding labels from a dataset\n",
    "    '''\n",
    "    assert len(data) == len(labels)\n",
    "\n",
    "    perm = torch.randperm(len(data))\n",
    "    return data[perm][:n], labels[perm][:n]\n",
    "\n",
    "def images_to_probs(net, images):\n",
    "    '''\n",
    "    Generates predictions and corresponding probabilities from a trained\n",
    "    network and a list of images\n",
    "    '''\n",
    "    output = net(images)\n",
    "    # convert output probabilities to predicted class\n",
    "    _, preds_tensor = torch.max(output, 1)\n",
    "    preds = np.squeeze(preds_tensor.cpu().numpy())\n",
    "    return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]\n",
    "\n",
    "def add_pr_curve_tensorboard(class_index, test_probs, test_preds, global_step=0):\n",
    "    '''\n",
    "    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n",
    "    precision-recall curve\n",
    "    '''\n",
    "    tensorboard_preds = test_preds == class_index\n",
    "    tensorboard_probs = test_probs[:, class_index]\n",
    "\n",
    "    writer.add_pr_curve(classes[class_index],\n",
    "                        tensorboard_preds,\n",
    "                        tensorboard_probs,\n",
    "                        global_step=global_step)\n",
    "#     writer.close()\n",
    "\n",
    "def accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "def plot_classes_preds(net, images, labels):\n",
    "    '''\n",
    "    Generates matplotlib Figure using a trained network, along with images\n",
    "    and labels from a batch, that shows the network's top prediction along\n",
    "    with its probability, alongside the actual label, coloring this\n",
    "    information based on whether the prediction was correct or not.\n",
    "    Uses the \"images_to_probs\" function.\n",
    "    '''\n",
    "    preds, probs = images_to_probs(net, images)\n",
    "    # plot the images in the batch, along with predicted and true labels\n",
    "    fig = plt.figure(figsize=(12, 48))\n",
    "    for idx in np.arange(4):\n",
    "        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n",
    "        matplotlib_imshow(images[idx], one_channel=True)\n",
    "        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
    "            classes[idx],\n",
    "            probs[idx] * 100.0,\n",
    "            classes[labels[idx]]),\n",
    "            color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))\n",
    "    return fig\n",
    "\n",
    "def get_data_set(train,dataset):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(255),\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "\n",
    "    # datasets\n",
    "    if dataset == 'mnist':\n",
    "        return torchvision.datasets.MNIST('./data',\n",
    "            download=True,\n",
    "            train=train,\n",
    "            transform=transform)\n",
    "    else:\n",
    "        print(f\"ERROR LOADING DATASET FOR: {dataset}\")\n",
    "    \n",
    "def get_class_constants(dataset):\n",
    "    if dataset == 'mnist':\n",
    "        return ('zero', 'one', 'two', 'three', 'four','five', 'six', 'seven', 'eight', 'nine')\n",
    "    elif dataset == 'cifar':\n",
    "        return ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck') \n",
    "    else:\n",
    "        print(f\"ERROR LOADING CLASS CONSTANTS FOR: {dataset}\")        \n",
    "\n",
    "def get_model_object(num_classes):\n",
    "#     if modeltype == 'alexnet':\n",
    "    return torchvision.models.alexnet(num_classes)\n",
    "\n",
    "        \n",
    "def setup_tensorboard_for_experiment(exp_name):\n",
    "    # writer = SummaryWriter('runs/reg_mnist_experiment_1')\n",
    "    writer = SummaryWriter('runs/' + exp_name)\n",
    "    # get some random training images\n",
    "    dataiter = iter(trainloader)\n",
    "    images, labels = dataiter.next()\n",
    "\n",
    "    images = images.to(device_name)\n",
    "    labels = labels.to(device_name)\n",
    "\n",
    "    # create grid of images\n",
    "    img_grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "    # show images\n",
    "    matplotlib_imshow(img_grid.cpu(), one_channel=True)\n",
    "\n",
    "    # write to tensorboard\n",
    "    writer.add_image('four_mnist_images', img_grid)\n",
    "\n",
    "    # add graph?\n",
    "    # writer.add_graph(net, images)\n",
    "    writer.close()\n",
    "\n",
    "    # select random images and their target indices\n",
    "    images, labels = select_n_random(trainset.data, trainset.targets)\n",
    "\n",
    "    # get the class labels for each image\n",
    "    class_labels = [classes[lab] for lab in labels]\n",
    "\n",
    "    # log embeddings\n",
    "    features = images.view(-1, 28 * 28)\n",
    "    writer.add_embedding(features,\n",
    "                        metadata=class_labels,\n",
    "                        label_img=images.unsqueeze(1))\n",
    "#     writer.close()\n",
    "    return writer\n",
    "\n",
    "def save_model(exp_name):\n",
    "#     print(str('./trained_models/'+exp_name+'.pth'))\n",
    "    torch.save(net.state_dict(), str('./trained_models/'+exp_name+'.pth'))\n",
    "    \n",
    "def train_model(exp_name,num_epochs,device_name,writer):\n",
    "    DELTA = 1e-3\n",
    "\n",
    "    running_loss = 0.0\n",
    "    losses = []\n",
    "    top1_acc=[]\n",
    "    virtual_batch_rate = VIRTUAL_BATCH_SIZE/BATCH_SIZE\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "            images, target = data\n",
    "            images = images.to(device_name)\n",
    "            target = target.to(device_name)\n",
    "\n",
    "            output = net(images)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            preds = np.argmax(output.detach().cpu().numpy(), axis=1)\n",
    "            labels = target.detach().cpu().numpy()\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc = accuracy(preds, labels)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            top1_acc.append(acc)\n",
    "            running_loss += loss.item() \n",
    "            loss.backward()\n",
    "            if dp:\n",
    "                if ((i + 1) % virtual_batch_rate == 0) or ((i + 1) == len(trainloader)):\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                else:\n",
    "                    optimizer.virtual_step() # take a virtual step\n",
    "            else:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "            if i % 200 == 199:\n",
    "                # If you will be training over a large number of epochs/dataset\n",
    "                # it may be helpful to save the progress of training periodically.\n",
    "                # Do that by uncommenting the line below:\n",
    "#                 save_model(exp_name) # uncomment to save progress during training\n",
    "                if dp:\n",
    "                    epsilon, best_alpha = optimizer.privacy_engine.get_privacy_spent(DELTA)\n",
    "                    print(\n",
    "                        f\"\\tTrain Epoch: {epoch} \\t\"\n",
    "                        f\"Loss: {np.mean(losses):.6f} \"\n",
    "                        f\"Acc@1: {np.mean(top1_acc) * 100:.6f} \"\n",
    "                        f\"(ε = {epsilon:.2f}, δ = {DELTA})\"\n",
    "                    )\n",
    "                else:\n",
    "                    print(\n",
    "                        f\"\\tTrain Epoch: {epoch} \\t\"\n",
    "                        f\"Loss: {np.mean(losses):.6f} \"\n",
    "                        f\"Acc@1: {np.mean(top1_acc) * 100:.6f} \"\n",
    "                    )\n",
    "                    \n",
    "                # Here we output Training metrics to TensorBoard so we can see \n",
    "                # how the loss is changing..\n",
    "                writer.add_scalar('training loss',running_loss/200,epoch * len(trainloader) + i)\n",
    "\n",
    "                # Pass tensorboard a set of predicted images, to demonstrate class accuracy during training\n",
    "                writer.add_figure('predictions vs. actuals',plot_classes_preds(net, images, labels),global_step=epoch * len(trainloader) + i)\n",
    "                running_loss = 0.0\n",
    "    \n",
    "    save_model(exp_name) \n",
    "    writer.close()\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following cell contains variable names that are relevant in the training of the model\n",
    " - `experiment_name`: Name given to the saved model, and also how it will be named in TensorBoard\n",
    " - `experiment_n`: Tracks the current experiment we are running - note change this or will overwrite trained models\n",
    " - `dataset`: Name of the preloaded PyTorch dataset to experiment with\n",
    " - `dp`: Should the model be trained using DP?\n",
    " - `device_name`: 'cpu' or 'cuda', specifies torch device\n",
    " - `LR`: learning rate for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "experiment_name = ''\n",
    "experiment_n= 0\n",
    "dataset     = 'mnist'  # not being used currently\n",
    "dp          = True\n",
    "device_name = 'cuda'\n",
    "\n",
    "BATCH_SIZE=8\n",
    "VIRTUAL_BATCH_SIZE=128\n",
    "LR = 9e-4\n",
    "m = .9\n",
    "\n",
    "# DP Engine parameters\n",
    "NOISE_MULT = 1.5\n",
    "MAX_GRAD_NORM = 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This cell is used to load models for display in TensorBoard\n",
    "\n",
    "# Get training and testing dataset\n",
    "trainset=get_data_set(True,dataset)\n",
    "testset=get_data_set(False,dataset)\n",
    "\n",
    "# Get training and testing dataloader\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,shuffle=False, num_workers=2)\n",
    "\n",
    "# constant for classes\n",
    "classes = get_class_constants(dataset)\n",
    "\n",
    "# get the model\n",
    "net = get_model_object(len(classes)).to(device_name)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=LR, momentum=m)\n",
    "# if dp:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impacts on performance\n",
    "There are a few key items that seems to have the most notable impact on performance. First, memory required to load a single training batch into memory during training. \n",
    "Image resolution and batch size play an important role in the amount of memory required to train the network. Similarly, learning rates and number of training epochs impacts the \n",
    "overall performance of the network after training. The differentially private model on average tended to require up to twice as many training epochs to converge to the same \n",
    "value using the same training data and network structure, just adding differentialy privacy to the network's optimizer. Running the code below demonstrates the different rates \n",
    "at which the losses of the differentially private models converge relative to one another. Likewise, learning rates can be adjusted such to demonstrate how learning rates \n",
    "impact training in both differentially private and vanilla models.\n",
    "\n",
    "More specific to the differentially private models, `NOISE_MULT` and `MAX_GRAD_NORM` are parameters that are relevant while training differentially private models. The optimizer \n",
    "uses these parameters to determine how exactly it should apply convolutions and transformations that are part of performing differentially private gradient optimization. From our \n",
    "experiments it seems that a higher `NOISE_MULT`, (and furthermore higher level of noise applied during DP gradient optimization) tended to perform better than models using smaller \n",
    "values.\n",
    "\n",
    "When the cells below are executing, make sure you have tensorboard running using: `tensorboard --logdirs=runs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist_0_dp_874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krizzly/school/5152/project/torchenv2/lib/python3.7/site-packages/opacus/privacy_engine.py:113: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  \"Secure RNG turned off. This is perfectly fine for experimentation as it allows \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Epoch: 0 \tLoss: 11.571669 Acc@1: 0.000000 (ε = 0.26, δ = 0.001)\n",
      "\tTrain Epoch: 0 \tLoss: 10.192690 Acc@1: 0.000000 (ε = 0.27, δ = 0.001)\n",
      "\tTrain Epoch: 0 \tLoss: 9.458576 Acc@1: 0.000000 (ε = 0.27, δ = 0.001)\n",
      "\tTrain Epoch: 0 \tLoss: 8.914884 Acc@1: 0.031250 (ε = 0.27, δ = 0.001)\n",
      "\tTrain Epoch: 0 \tLoss: 8.428239 Acc@1: 0.187500 (ε = 0.27, δ = 0.001)\n",
      "\tTrain Epoch: 0 \tLoss: 7.972111 Acc@1: 0.906250 (ε = 0.27, δ = 0.001)\n",
      "\tTrain Epoch: 0 \tLoss: 7.556262 Acc@1: 2.205357 (ε = 0.27, δ = 0.001)\n",
      "\tTrain Epoch: 0 \tLoss: 7.197550 Acc@1: 3.679688 (ε = 0.27, δ = 0.001)\n",
      "\tTrain Epoch: 0 \tLoss: 6.880509 Acc@1: 5.270833 (ε = 0.27, δ = 0.001)\n",
      "\tTrain Epoch: 0 \tLoss: 6.606619 Acc@1: 6.675000 (ε = 0.27, δ = 0.001)\n",
      "\tTrain Epoch: 0 \tLoss: 6.350371 Acc@1: 8.056818 (ε = 0.27, δ = 0.001)\n",
      "\tTrain Epoch: 0 \tLoss: 6.127727 Acc@1: 9.291667 (ε = 0.27, δ = 0.001)\n",
      "\tTrain Epoch: 0 \tLoss: 5.916571 Acc@1: 10.625000 (ε = 0.27, δ = 0.001)\n",
      "\tTrain Epoch: 0 \tLoss: 5.730508 Acc@1: 11.638393 (ε = 0.27, δ = 0.001)\n",
      "\tTrain Epoch: 0 \tLoss: 5.556785 Acc@1: 12.587500 (ε = 0.27, δ = 0.001)\n",
      "\tTrain Epoch: 0 \tLoss: 5.397621 Acc@1: 13.425781 (ε = 0.27, δ = 0.001)\n",
      "\tTrain Epoch: 0 \tLoss: 5.247063 Acc@1: 14.238971 (ε = 0.27, δ = 0.001)\n",
      "\tTrain Epoch: 0 \tLoss: 5.104980 Acc@1: 15.211806 (ε = 0.27, δ = 0.001)\n",
      "\tTrain Epoch: 0 \tLoss: 4.972332 Acc@1: 15.973684 (ε = 0.27, δ = 0.001)\n",
      "\tTrain Epoch: 0 \tLoss: 4.845283 Acc@1: 16.825000 (ε = 0.28, δ = 0.001)\n",
      "\tTrain Epoch: 0 \tLoss: 4.727366 Acc@1: 17.532738 (ε = 0.28, δ = 0.001)\n",
      "\tTrain Epoch: 0 \tLoss: 4.614788 Acc@1: 18.292614 (ε = 0.28, δ = 0.001)\n",
      "\tTrain Epoch: 0 \tLoss: 4.507395 Acc@1: 18.967391 (ε = 0.28, δ = 0.001)\n",
      "\tTrain Epoch: 0 \tLoss: 4.407166 Acc@1: 19.601562 (ε = 0.28, δ = 0.001)\n",
      "\tTrain Epoch: 0 \tLoss: 4.309027 Acc@1: 20.347500 (ε = 0.28, δ = 0.001)\n",
      "\tTrain Epoch: 0 \tLoss: 4.219835 Acc@1: 20.939904 (ε = 0.28, δ = 0.001)\n",
      "\tTrain Epoch: 0 \tLoss: 4.134131 Acc@1: 21.576389 (ε = 0.28, δ = 0.001)\n",
      "\tTrain Epoch: 0 \tLoss: 4.050731 Acc@1: 22.279018 (ε = 0.28, δ = 0.001)\n",
      "\tTrain Epoch: 0 \tLoss: 3.972164 Acc@1: 22.954741 (ε = 0.28, δ = 0.001)\n",
      "\tTrain Epoch: 0 \tLoss: 3.898860 Acc@1: 23.508333 (ε = 0.28, δ = 0.001)\n",
      "\tTrain Epoch: 0 \tLoss: 3.827948 Acc@1: 24.118952 (ε = 0.28, δ = 0.001)\n",
      "\tTrain Epoch: 0 \tLoss: 3.764406 Acc@1: 24.617188 (ε = 0.28, δ = 0.001)\n",
      "\tTrain Epoch: 0 \tLoss: 3.699221 Acc@1: 25.236742 (ε = 0.28, δ = 0.001)\n",
      "\tTrain Epoch: 0 \tLoss: 3.640239 Acc@1: 25.720588 (ε = 0.28, δ = 0.001)\n",
      "\tTrain Epoch: 0 \tLoss: 3.581598 Acc@1: 26.326786 (ε = 0.28, δ = 0.001)\n",
      "\tTrain Epoch: 0 \tLoss: 3.527714 Acc@1: 26.828125 (ε = 0.28, δ = 0.001)\n",
      "\tTrain Epoch: 0 \tLoss: 3.475691 Acc@1: 27.329392 (ε = 0.28, δ = 0.001)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krizzly/school/5152/project/torchenv2/lib/python3.7/site-packages/opacus/privacy_engine.py:273: UserWarning: PrivacyEngine expected a batch of size 128 but the last step received a batch of size 96. This means that the privacy analysis will be a bit more pessimistic. You can set `drop_last = True` in your PyTorch dataloader to avoid this problem completely\n",
      "  f\"PrivacyEngine expected a batch of size {self.batch_size} \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Epoch: 1 \tLoss: 3.401680 Acc@1: 28.086039 (ε = 0.28, δ = 0.001)\n",
      "\tTrain Epoch: 1 \tLoss: 3.353613 Acc@1: 28.607595 (ε = 0.28, δ = 0.001)\n",
      "\tTrain Epoch: 1 \tLoss: 3.308180 Acc@1: 29.089506 (ε = 0.28, δ = 0.001)\n",
      "\tTrain Epoch: 1 \tLoss: 3.263953 Acc@1: 29.590361 (ε = 0.29, δ = 0.001)\n",
      "\tTrain Epoch: 1 \tLoss: 3.221672 Acc@1: 30.076471 (ε = 0.29, δ = 0.001)\n",
      "\tTrain Epoch: 1 \tLoss: 3.181039 Acc@1: 30.544540 (ε = 0.29, δ = 0.001)\n",
      "\tTrain Epoch: 1 \tLoss: 3.141507 Acc@1: 31.011236 (ε = 0.29, δ = 0.001)\n",
      "\tTrain Epoch: 1 \tLoss: 3.102984 Acc@1: 31.510989 (ε = 0.29, δ = 0.001)\n",
      "\tTrain Epoch: 1 \tLoss: 3.065875 Acc@1: 31.977151 (ε = 0.29, δ = 0.001)\n",
      "\tTrain Epoch: 1 \tLoss: 3.030515 Acc@1: 32.414474 (ε = 0.29, δ = 0.001)\n",
      "\tTrain Epoch: 1 \tLoss: 2.995546 Acc@1: 32.858247 (ε = 0.29, δ = 0.001)\n",
      "\tTrain Epoch: 1 \tLoss: 2.961897 Acc@1: 33.279040 (ε = 0.29, δ = 0.001)\n",
      "\tTrain Epoch: 1 \tLoss: 2.929677 Acc@1: 33.673267 (ε = 0.29, δ = 0.001)\n",
      "\tTrain Epoch: 1 \tLoss: 2.897170 Acc@1: 34.141990 (ε = 0.29, δ = 0.001)\n",
      "\tTrain Epoch: 1 \tLoss: 2.865237 Acc@1: 34.576190 (ε = 0.29, δ = 0.001)\n",
      "\tTrain Epoch: 1 \tLoss: 2.835397 Acc@1: 35.022196 (ε = 0.29, δ = 0.001)\n",
      "\tTrain Epoch: 1 \tLoss: 2.805716 Acc@1: 35.464450 (ε = 0.29, δ = 0.001)\n",
      "\tTrain Epoch: 1 \tLoss: 2.775986 Acc@1: 35.941441 (ε = 0.29, δ = 0.001)\n",
      "\tTrain Epoch: 1 \tLoss: 2.747567 Acc@1: 36.377212 (ε = 0.29, δ = 0.001)\n",
      "\tTrain Epoch: 1 \tLoss: 2.720765 Acc@1: 36.788043 (ε = 0.29, δ = 0.001)\n",
      "\tTrain Epoch: 1 \tLoss: 2.694083 Acc@1: 37.190171 (ε = 0.29, δ = 0.001)\n",
      "\tTrain Epoch: 1 \tLoss: 2.669003 Acc@1: 37.555672 (ε = 0.29, δ = 0.001)\n",
      "\tTrain Epoch: 1 \tLoss: 2.643322 Acc@1: 37.956612 (ε = 0.29, δ = 0.001)\n",
      "\tTrain Epoch: 1 \tLoss: 2.617968 Acc@1: 38.360772 (ε = 0.29, δ = 0.001)\n",
      "\tTrain Epoch: 1 \tLoss: 2.593682 Acc@1: 38.746000 (ε = 0.30, δ = 0.001)\n",
      "\tTrain Epoch: 1 \tLoss: 2.569719 Acc@1: 39.150591 (ε = 0.30, δ = 0.001)\n",
      "\tTrain Epoch: 1 \tLoss: 2.546770 Acc@1: 39.533915 (ε = 0.30, δ = 0.001)\n",
      "\tTrain Epoch: 1 \tLoss: 2.523839 Acc@1: 39.930344 (ε = 0.30, δ = 0.001)\n",
      "\tTrain Epoch: 1 \tLoss: 2.502381 Acc@1: 40.286654 (ε = 0.30, δ = 0.001)\n",
      "\tTrain Epoch: 1 \tLoss: 2.480148 Acc@1: 40.655556 (ε = 0.30, δ = 0.001)\n",
      "\tTrain Epoch: 1 \tLoss: 2.458486 Acc@1: 41.044708 (ε = 0.30, δ = 0.001)\n",
      "\tTrain Epoch: 1 \tLoss: 2.438311 Acc@1: 41.402878 (ε = 0.30, δ = 0.001)\n",
      "\tTrain Epoch: 1 \tLoss: 2.418097 Acc@1: 41.761525 (ε = 0.30, δ = 0.001)\n",
      "\tTrain Epoch: 1 \tLoss: 2.398245 Acc@1: 42.114510 (ε = 0.30, δ = 0.001)\n",
      "\tTrain Epoch: 1 \tLoss: 2.378805 Acc@1: 42.445690 (ε = 0.30, δ = 0.001)\n",
      "\tTrain Epoch: 1 \tLoss: 2.360068 Acc@1: 42.790816 (ε = 0.30, δ = 0.001)\n",
      "\tTrain Epoch: 1 \tLoss: 2.340847 Acc@1: 43.130872 (ε = 0.30, δ = 0.001)\n",
      "\tTrain Epoch: 2 \tLoss: 2.313330 Acc@1: 43.614309 (ε = 0.30, δ = 0.001)\n",
      "\tTrain Epoch: 2 \tLoss: 2.294823 Acc@1: 43.954545 (ε = 0.30, δ = 0.001)\n",
      "\tTrain Epoch: 2 \tLoss: 2.277165 Acc@1: 44.277244 (ε = 0.30, δ = 0.001)\n",
      "\tTrain Epoch: 2 \tLoss: 2.260449 Acc@1: 44.595728 (ε = 0.30, δ = 0.001)\n",
      "\tTrain Epoch: 2 \tLoss: 2.243324 Acc@1: 44.930469 (ε = 0.30, δ = 0.001)\n",
      "\tTrain Epoch: 2 \tLoss: 2.226028 Acc@1: 45.253086 (ε = 0.30, δ = 0.001)\n",
      "\tTrain Epoch: 2 \tLoss: 2.209609 Acc@1: 45.572409 (ε = 0.30, δ = 0.001)\n",
      "\tTrain Epoch: 2 \tLoss: 2.193177 Acc@1: 45.904367 (ε = 0.30, δ = 0.001)\n",
      "\tTrain Epoch: 2 \tLoss: 2.177254 Acc@1: 46.226935 (ε = 0.31, δ = 0.001)\n",
      "\tTrain Epoch: 2 \tLoss: 2.161625 Acc@1: 46.532353 (ε = 0.31, δ = 0.001)\n",
      "\tTrain Epoch: 2 \tLoss: 2.146810 Acc@1: 46.829215 (ε = 0.31, δ = 0.001)\n",
      "\tTrain Epoch: 2 \tLoss: 2.131934 Acc@1: 47.117816 (ε = 0.31, δ = 0.001)\n",
      "\tTrain Epoch: 2 \tLoss: 2.117778 Acc@1: 47.404119 (ε = 0.31, δ = 0.001)\n",
      "\tTrain Epoch: 2 \tLoss: 2.102768 Acc@1: 47.713483 (ε = 0.31, δ = 0.001)\n",
      "\tTrain Epoch: 2 \tLoss: 2.087990 Acc@1: 48.016667 (ε = 0.31, δ = 0.001)\n",
      "\tTrain Epoch: 2 \tLoss: 2.074242 Acc@1: 48.304258 (ε = 0.31, δ = 0.001)\n",
      "\tTrain Epoch: 2 \tLoss: 2.060526 Acc@1: 48.588315 (ε = 0.31, δ = 0.001)\n",
      "\tTrain Epoch: 2 \tLoss: 2.046985 Acc@1: 48.879704 (ε = 0.31, δ = 0.001)\n",
      "\tTrain Epoch: 2 \tLoss: 2.033546 Acc@1: 49.156915 (ε = 0.31, δ = 0.001)\n",
      "\tTrain Epoch: 2 \tLoss: 2.020247 Acc@1: 49.453947 (ε = 0.31, δ = 0.001)\n",
      "\tTrain Epoch: 2 \tLoss: 2.007383 Acc@1: 49.719401 (ε = 0.31, δ = 0.001)\n",
      "\tTrain Epoch: 2 \tLoss: 1.994574 Acc@1: 50.005799 (ε = 0.31, δ = 0.001)\n",
      "\tTrain Epoch: 2 \tLoss: 1.981383 Acc@1: 50.292092 (ε = 0.31, δ = 0.001)\n",
      "\tTrain Epoch: 2 \tLoss: 1.968827 Acc@1: 50.553030 (ε = 0.31, δ = 0.001)\n",
      "\tTrain Epoch: 2 \tLoss: 1.957059 Acc@1: 50.808125 (ε = 0.31, δ = 0.001)\n",
      "\tTrain Epoch: 2 \tLoss: 1.944967 Acc@1: 51.073639 (ε = 0.31, δ = 0.001)\n",
      "\tTrain Epoch: 2 \tLoss: 1.933640 Acc@1: 51.319853 (ε = 0.31, δ = 0.001)\n",
      "\tTrain Epoch: 2 \tLoss: 1.922347 Acc@1: 51.564320 (ε = 0.31, δ = 0.001)\n",
      "\tTrain Epoch: 2 \tLoss: 1.911571 Acc@1: 51.803486 (ε = 0.31, δ = 0.001)\n",
      "\tTrain Epoch: 2 \tLoss: 1.900108 Acc@1: 52.064881 (ε = 0.31, δ = 0.001)\n",
      "\tTrain Epoch: 2 \tLoss: 1.889482 Acc@1: 52.303656 (ε = 0.32, δ = 0.001)\n",
      "\tTrain Epoch: 2 \tLoss: 1.879499 Acc@1: 52.535047 (ε = 0.32, δ = 0.001)\n",
      "\tTrain Epoch: 2 \tLoss: 1.869032 Acc@1: 52.768519 (ε = 0.32, δ = 0.001)\n",
      "\tTrain Epoch: 2 \tLoss: 1.858467 Acc@1: 53.015482 (ε = 0.32, δ = 0.001)\n",
      "\tTrain Epoch: 2 \tLoss: 1.848448 Acc@1: 53.247159 (ε = 0.32, δ = 0.001)\n",
      "\tTrain Epoch: 2 \tLoss: 1.838594 Acc@1: 53.481419 (ε = 0.32, δ = 0.001)\n",
      "\tTrain Epoch: 2 \tLoss: 1.828390 Acc@1: 53.726562 (ε = 0.32, δ = 0.001)\n",
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "privacy_engine = PrivacyEngine(\n",
    "    net,\n",
    "    batch_size=VIRTUAL_BATCH_SIZE,\n",
    "    sample_size=len(trainset),\n",
    "    alphas=range(2,32),\n",
    "    noise_multiplier=NOISE_MULT,\n",
    "    max_grad_norm=MAX_GRAD_NORM\n",
    ")   \n",
    "privacy_engine.attach(optimizer)\n",
    "LR = 9e-5\n",
    "experiment_name = f'{dataset}_{experiment_n}_dp'+ '_' +str(randint(100, 999))\n",
    "print(experiment_name)\n",
    "writer = setup_tensorboard_for_experiment(experiment_name)\n",
    "train_model(experiment_name,3,device_name,writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist_0_788\n",
      "\tTrain Epoch: 0 \tLoss: 1.631808 Acc@1: 57.750000 \n",
      "\tTrain Epoch: 0 \tLoss: 0.955851 Acc@1: 74.906250 \n",
      "\tTrain Epoch: 0 \tLoss: 0.712178 Acc@1: 81.125000 \n",
      "\tTrain Epoch: 0 \tLoss: 0.571267 Acc@1: 84.750000 \n",
      "\tTrain Epoch: 0 \tLoss: 0.476011 Acc@1: 87.187500 \n",
      "\tTrain Epoch: 0 \tLoss: 0.419954 Acc@1: 88.677083 \n"
     ]
    }
   ],
   "source": [
    "# Trains a model WITHOUT differential privacy enabled\n",
    "LR= 9e-5\n",
    "dp = False\n",
    "experiment_name = f'{dataset}_{experiment_n}'+ '_' +str(randint(100, 999))\n",
    "print(experiment_name)\n",
    "writer = setup_tensorboard_for_experiment(experiment_name)\n",
    "train_model(experiment_name,3,device_name,writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
